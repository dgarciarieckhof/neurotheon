{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe60218",
   "metadata": {},
   "source": [
    "# Neurotheon - Reinforcement Learning on Humanoid-v5\n",
    "\n",
    "```bash\n",
    "| \\ | | _____ _____ _ | | _ __ _ __()_ __ __ _\n",
    "| | |/ _ \\ / / _ \\ '| _| ' | '| | '_ \\ / ` |\n",
    "| |\\ | __/> < __/ | | || |) | | | | | | | (| |_\n",
    "|| _|___//__|| _| ./|| ||| ||_, ()\n",
    "|| |__/\n",
    "NEUROTHEON Â· Project Code: NRTH\n",
    "```\n",
    "\n",
    "**Goal:** Train a bipedal humanoid to walk and balance using a custom reward function.\n",
    "Using PPO, multiple parallel environments, and a shaped reward that encourages:\n",
    "- Moving forward\n",
    "- Staying upright\n",
    "- Smooth actions\n",
    "- Sustained walking\n",
    "\n",
    "I'm logging reward components to TensorBoard for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from IPython.display import Video, display\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0360e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory if needed\n",
    "dirpath = os.path.dirname(os.getcwd())\n",
    "os.chdir(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "print(\"Torch CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and logs\n",
    "log_dir = \"./logs/ppo_humanoid/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "model_dir = \"./models\"\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parallel environment setup (16 workers)\n",
    "NUM_ENVS = min(os.cpu_count() // 2, 16)\n",
    "\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(\"Humanoid-v5\")\n",
    "        return Monitor(env)\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ff6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom reward shaping wrapper with survival, forward movement, posture, and smoothness\n",
    "def custom_reward_wrapper(env):\n",
    "    class CustomReward(gym.Wrapper):\n",
    "        def __init__(self, env):\n",
    "            super().__init__(env)\n",
    "            self.episode_length = 0\n",
    "            self.past_lengths = deque(maxlen=50)  # Rolling window\n",
    "            self.last_bonus = 0.0\n",
    "\n",
    "        def reset(self, **kwargs):\n",
    "            self.episode_length = 0\n",
    "            return self.env.reset(**kwargs)\n",
    "\n",
    "        def step(self, action):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "            forward = info.get(\"reward_forward\", 0)\n",
    "            survive = info.get(\"reward_survive\", 0)\n",
    "            ctrl = info.get(\"reward_ctrl\", 0)\n",
    "\n",
    "            shaped = (\n",
    "                2.5 * forward +\n",
    "                1.0 * survive -\n",
    "                0.25 * ctrl\n",
    "            )\n",
    "\n",
    "            # Upright posture\n",
    "            torso_angle = obs[2] if len(obs) > 2 else 0\n",
    "            upright_bonus = 0.5 if abs(torso_angle) < 0.2 else 0.0\n",
    "            shaped += upright_bonus\n",
    "\n",
    "            self.episode_length += 1\n",
    "            walk_bonus = 0.0\n",
    "\n",
    "            if terminated or truncated:\n",
    "                # Add to history\n",
    "                self.past_lengths.append(self.episode_length)\n",
    "\n",
    "                # Compute moving average\n",
    "                avg_len = np.mean(self.past_lengths) if self.past_lengths else 0\n",
    "\n",
    "                # Bonus if current episode outperforms average\n",
    "                if self.episode_length > avg_len:\n",
    "                    walk_bonus = 0.5\n",
    "\n",
    "                shaped += walk_bonus\n",
    "                self.last_bonus = walk_bonus  # log for visualization\n",
    "\n",
    "            # Logging\n",
    "            info[\"shaped_reward\"] = shaped\n",
    "            info[\"bonus_upright\"] = upright_bonus\n",
    "            info[\"bonus_duration\"] = self.last_bonus  # only gets non-zero at end\n",
    "            info[\"ep_len_avg\"] = np.mean(self.past_lengths) if self.past_lengths else 0\n",
    "\n",
    "            return obs, shaped, terminated, truncated, info\n",
    "\n",
    "    return CustomReward(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply reward wrapper to parallel environments\n",
    "env_wrapped = SubprocVecEnv([\n",
    "    lambda: custom_reward_wrapper(make_env()()) for _ in range(NUM_ENVS)\n",
    "])\n",
    "\n",
    "n_steps = 2048  # good\n",
    "batch_size = n_steps * NUM_ENVS // 4\n",
    "\n",
    "# PPO with good hyperparams for humanoid locomotion\n",
    "device = \"cpu\"\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env_wrapped,\n",
    "    verbose=0,\n",
    "    tensorboard_log=log_dir,\n",
    "    device=device,\n",
    "    n_steps=n_steps,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=10,\n",
    "    learning_rate=2.5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to log custom reward components to TensorBoard\n",
    "class RewardLoggingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "        for info in infos:\n",
    "            if \"shaped_reward\" in info:\n",
    "                self.logger.record(\"custom/reward_total\", info[\"shaped_reward\"])\n",
    "                self.logger.record(\"custom/reward_forward\", info.get(\"reward_forward\", 0))\n",
    "                self.logger.record(\"custom/reward_survive\", info.get(\"reward_survive\", 0))\n",
    "                self.logger.record(\"custom/reward_ctrl\", info.get(\"reward_ctrl\", 0))\n",
    "                self.logger.record(\"custom/bonus_upright\", info.get(\"bonus_upright\", 0))\n",
    "                self.logger.record(\"custom/bonus_duration\", info.get(\"bonus_duration\", 0))\n",
    "                self.logger.record(\"custom/ep_len_avg\", info.get(\"ep_len_avg\", 0))\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ac89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with callback\n",
    "callback = RewardLoggingCallback()\n",
    "model.learn(total_timesteps=1_000_000, callback=callback, progress_bar=True)\n",
    "model.save(f\"{model_dir}/neurotheon_ppo_1m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now launch TensorBoard in your terminal (not notebook):\n",
    "# --------------------------------------------------------\n",
    "# source .venv/bin/activate\n",
    "# tensorboard --logdir=./logs/ppo_humanoid --port=6006\n",
    "# Open http://localhost:6006 in your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7813191",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ade899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rewards(tb_path):\n",
    "    ea = EventAccumulator(tb_path)\n",
    "    ea.Reload()\n",
    "    vals = ea.Scalars(\"rollout/ep_rew_mean\")\n",
    "    steps = [v.step for v in vals]\n",
    "    rews = [v.value for v in vals]\n",
    "    return steps, rews\n",
    "\n",
    "def render_milestone(model, step, reward_val):\n",
    "    env = gym.make(\"Humanoid-v5\", render_mode=\"rgb_array\")\n",
    "    obs, _ = env.reset()\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(250):\n",
    "        frame = env.render()\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, _, done, trunc, _ = env.step(action)\n",
    "        frames.append(frame)\n",
    "        if done or trunc:\n",
    "            break\n",
    "\n",
    "    return frames, reward_val\n",
    "\n",
    "def draw_reward_plot(steps, rewards, current_step):\n",
    "    fig, ax = plt.subplots(figsize=(9.6, 10.8))\n",
    "    ax.plot(steps, rewards, lw=3, color=\"lightblue\")\n",
    "    ax.axvline(current_step, ls=\"--\", color=\"white\")\n",
    "    ax.set_title(\"Reward over Time\", color=\"white\")\n",
    "    ax.set_facecolor(\"black\")\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('white')\n",
    "    ax.tick_params(colors='white')\n",
    "    fig.patch.set_facecolor(\"black\")\n",
    "    canvas = FigureCanvas(fig)\n",
    "    canvas.draw()\n",
    "    buf, (w, h) = canvas.print_to_buffer()\n",
    "    img = np.frombuffer(buf, dtype=np.uint8).reshape(h, w, 4)[:, :, :3]\n",
    "    plt.close(fig)\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "def build_frame(video_frame, plot_img, step, reward_val):\n",
    "    canvas = Image.new(\"RGB\", (1920, 1080), (0, 0, 0))\n",
    "    canvas.paste(plot_img, (0, 0))\n",
    "    canvas.paste(Image.fromarray(video_frame).resize((960, 1080)), (960, 0))\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "    font = ImageFont.load_default()\n",
    "    draw.text((1000, 60), f\"Step: {step:,}\", fill=\"white\", font=font)\n",
    "    draw.text((1000, 120), f\"Reward: {reward_val:.2f}\", fill=\"lightblue\", font=font)\n",
    "    return np.array(canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29364f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models/neurotheon_ppo_1m\")\n",
    "steps, rewards = load_rewards(\"logs/ppo_humanoid/PPO_1\")\n",
    "\n",
    "milestones = [0, 250_000, 500_000, 750_000, 1_000_000]\n",
    "frames_all = []\n",
    "\n",
    "for step in milestones:\n",
    "    reward_val = np.interp(step, steps, rewards)\n",
    "    vid_frames, _ = render_milestone(model, step, reward_val)\n",
    "    plot_img = draw_reward_plot(steps, rewards, step)\n",
    "\n",
    "    for vf in vid_frames:\n",
    "        combined = build_frame(vf, plot_img, step, reward_val)\n",
    "        frames_all.append(combined)\n",
    "\n",
    "clip = ImageSequenceClip(frames_all, fps=30)\n",
    "clip.write_videofile(f\"{\"videos\"}/humanoid_milestone_timelapse.mp4\", codec=\"libx264\")\n",
    "display(Video(f\"{\"videos\"}/humanoid_milestone_timelapse.mp4\", embed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b370c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
